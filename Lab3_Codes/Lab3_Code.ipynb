{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "SUHIT DATTA 50249271\n",
    "\n",
    "SOURAV RANU 50246451\n",
    "\n",
    "LAB 3 CSE 587"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Directory which consists the data \n",
    "dir = \"DIC_Project_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates a subdirectory list \n",
    "subDirNameList =[]\n",
    "for root, dirs, files in os.walk(dir, topdown=False):\n",
    "    for name in dirs:\n",
    "        subDirNameList.append(os.path.join(root, name))\n",
    "\n",
    "#creates a list of dictionary elements of each category type         \n",
    "listRDD = []\n",
    "dictRDDElementsAsMap ={}\n",
    "for eachFolder in subDirNameList:\n",
    "    folderName = os.path.basename(eachFolder)\n",
    "    rdd = sc.wholeTextFiles(eachFolder)\n",
    "    listRDD.append(rdd)\n",
    "    dictElement = rdd.collectAsMap()\n",
    "    dictRDDElementsAsMap[folderName] = dictElement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to check if a string is blank or not \n",
    "def isNotBlank (myString):\n",
    "    if myString and myString.strip():\n",
    "        #myString is not None AND myString is not empty or blank\n",
    "        return True\n",
    "    #myString is None OR myString is empty or blank\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this creates a list. Each element of a list is basically a dictionary : \n",
    "    key = category and\n",
    "    value = text content  \n",
    "\"\"\"\n",
    "listAll = []\n",
    "for name,v1 in dictRDDElementsAsMap.items():\n",
    "    for key,value in v1.items():\n",
    "        dataDic = {}\n",
    "        if isNotBlank(value):\n",
    "            dataDic['category'] = name\n",
    "            dataDic['text'] = value \n",
    "            listAll.append(dataDic)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suhit\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:340: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    }
   ],
   "source": [
    "# this creates the dataframe from the list \n",
    "FULLdf = spark.createDataFrame(listAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[category: string, text: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splitting into training and test set \n",
    "training, test = FULLdf.randomSplit([0.8, 0.2], seed=7)\n",
    "training.cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+----------+\n",
      "|category|                text|label|               words|            filtered|         rawFeatures|            features|     rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+----------+\n",
      "|Business| Good Tuesday. He...|  2.0|[, good, tuesday....|[, good, tuesday....|(1000,[0,1,2,3,4,...|(1000,[0,1,2,3,4,...| [2.0,5.0,6.0,2.0]|[0.13333333333333...|       2.0|\n",
      "|Business|Brady Hill used h...|  2.0|[brady, hill, use...|[brady, hill, use...|(1000,[0,1,2,3,8,...|(1000,[0,1,2,3,8,...| [4.0,4.0,4.0,3.0]|[0.26666666666666...|       0.0|\n",
      "|Business|Good Wednesday. H...|  2.0|[good, wednesday....|[good, wednesday....|(1000,[0,1,2,3,4,...|(1000,[0,1,2,3,4,...| [6.0,5.0,4.0,0.0]|[0.4,0.3333333333...|       0.0|\n",
      "|Business|I didn’t mean to ...|  2.0|[i, didn’t, mean,...|[didn’t, mean, it...|(1000,[0,2,4,6,7,...|(1000,[0,2,4,6,7,...| [6.0,7.0,2.0,0.0]|[0.4,0.4666666666...|       1.0|\n",
      "|Business|Long before dawn ...|  2.0|[long, before, da...|[long, dawn, wind...|(1000,[0,1,2,4,5,...|(1000,[0,1,2,4,5,...| [4.0,5.0,3.0,3.0]|[0.26666666666666...|       1.0|\n",
      "|Business|Microsoft, more t...|  2.0|[microsoft,, more...|[microsoft,, trad...|(1000,[0,1,8,13,1...|(1000,[0,1,8,13,1...| [2.0,1.0,8.0,4.0]|[0.13333333333333...|       2.0|\n",
      "|Business|SAN JUAN, P.R. — ...|  2.0|[san, juan,, p.r....|[san, juan,, p.r....|(1000,[1,8,10,12,...|(1000,[1,8,10,12,...| [4.0,3.0,3.0,5.0]|[0.26666666666666...|       3.0|\n",
      "|Business|SHANGHAI — Want t...|  2.0|[shanghai, —, wan...|[shanghai, —, wan...|(1000,[0,3,4,8,15...|(1000,[0,3,4,8,15...| [4.0,3.0,7.0,1.0]|[0.26666666666666...|       2.0|\n",
      "|Business|SÃO PAULO, Brazil...|  2.0|[são, paulo,, bra...|[são, paulo,, bra...|(1000,[3,4,7,10,2...|(1000,[3,4,7,10,2...| [3.0,2.0,7.0,3.0]|[0.2,0.1333333333...|       2.0|\n",
      "|Business|TOKYO — The Japan...|  2.0|[tokyo, —, the, j...|[tokyo, —, japane...|(1000,[8,10,16,18...|(1000,[8,10,16,18...| [1.0,2.0,9.0,3.0]|[0.06666666666666...|       2.0|\n",
      "|Business|The economy grew ...|  2.0|[the, economy, gr...|[economy, grew, a...|(1000,[3,7,8,10,1...|(1000,[3,7,8,10,1...| [4.0,4.0,5.0,2.0]|[0.26666666666666...|       2.0|\n",
      "|Business|Wall Street was p...|  2.0|[wall, street, wa...|[wall, street, pr...|(1000,[3,25,34,36...|(1000,[3,25,34,36...| [1.0,2.0,7.0,5.0]|[0.06666666666666...|       2.0|\n",
      "|Politics|On Day 10 of a sc...|  0.0|[on, day, 10, of,...|[day, 10, scandal...|(1000,[0,1,2,5,6,...|(1000,[0,1,2,5,6,...|[10.0,4.0,0.0,1.0]|[0.66666666666666...|       0.0|\n",
      "|Politics|On Saturday, Rebe...|  0.0|[on, saturday,, r...|[saturday,, rebec...|(1000,[0,1,4,6,7,...|(1000,[0,1,4,6,7,...| [8.0,3.0,3.0,1.0]|[0.53333333333333...|       0.0|\n",
      "|Politics|PALM BEACH, Fla. ...|  0.0|[palm, beach,, fl...|[palm, beach,, fl...|(1000,[0,3,6,7,10...|(1000,[0,3,6,7,10...| [6.0,3.0,6.0,0.0]|   [0.4,0.2,0.4,0.0]|       0.0|\n",
      "|Politics|PHOENIX — Melinda...|  0.0|[phoenix, —, meli...|[phoenix, —, meli...|(1000,[0,1,3,4,7,...|(1000,[0,1,3,4,7,...| [8.0,2.0,4.0,1.0]|[0.53333333333333...|       0.0|\n",
      "|Politics|The driver was dr...|  0.0|[the, driver, was...|[driver, drunk,, ...|(1000,[1,2,3,4,5,...|(1000,[1,2,3,4,5,...| [8.0,1.0,5.0,1.0]|[0.53333333333333...|       0.0|\n",
      "|Politics|WASHINGTON — A fe...|  0.0|[washington, —, a...|[washington, —, h...|(1000,[0,1,3,4,5,...|(1000,[0,1,3,4,5,...| [4.0,4.0,3.0,4.0]|[0.26666666666666...|       0.0|\n",
      "|Politics|WASHINGTON — Even...|  0.0|[washington, —, e...|[washington, —, e...|(1000,[1,2,5,7,15...|(1000,[1,2,5,7,15...| [3.0,3.0,6.0,3.0]|   [0.2,0.2,0.4,0.2]|       2.0|\n",
      "|Politics|WASHINGTON — Harr...|  0.0|[washington, —, h...|[washington, —, h...|(1000,[0,2,3,13,1...|(1000,[0,2,3,13,1...| [5.0,6.0,1.0,3.0]|[0.33333333333333...|       1.0|\n",
      "+--------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test set accuracy = 0.7021276595744681\n"
     ]
    }
   ],
   "source": [
    "# applying Random Forest  \n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer , IDF , StringIndexer ,StopWordsRemover\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Configure an ML pipeline\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "stopwordsRemover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "hashingTF = HashingTF(inputCol=stopwordsRemover.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"features\")\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=15,  maxDepth=12)\n",
    "pipeline = Pipeline(stages=[indexer, tokenizer,stopwordsRemover, hashingTF,idf, rf])\n",
    "\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=4)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModelRF = crossval.fit(training)\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "predictions = cvModelRF.transform(test)\n",
    "predictions.show()\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "converter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\")\n",
    "converted = converter.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|category|                text|label|               words|            filtered|         rawFeatures|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|Business| Good Tuesday. He...|  2.0|[, good, tuesday....|[, good, tuesday....|(1000,[0,1,2,3,4,...|(1000,[0,1,2,3,4,...|[-7506.8131448156...|[1.61820182577732...|       2.0|\n",
      "|Business|Brady Hill used h...|  2.0|[brady, hill, use...|[brady, hill, use...|(1000,[0,1,2,3,8,...|(1000,[0,1,2,3,8,...|[-4785.4573792874...|[2.64678005613299...|       1.0|\n",
      "|Business|Good Wednesday. H...|  2.0|[good, wednesday....|[good, wednesday....|(1000,[0,1,2,3,4,...|(1000,[0,1,2,3,4,...|[-10732.631692611...|[3.65314603578155...|       2.0|\n",
      "|Business|I didn’t mean to ...|  2.0|[i, didn’t, mean,...|[didn’t, mean, it...|(1000,[0,2,4,6,7,...|(1000,[0,2,4,6,7,...|[-3860.7644305603...|[5.58602358666721...|       1.0|\n",
      "|Business|Long before dawn ...|  2.0|[long, before, da...|[long, dawn, wind...|(1000,[0,1,2,4,5,...|(1000,[0,1,2,4,5,...|[-7036.6996978978...|[5.16260372878484...|       1.0|\n",
      "|Business|Microsoft, more t...|  2.0|[microsoft,, more...|[microsoft,, trad...|(1000,[0,1,8,13,1...|(1000,[0,1,8,13,1...|[-1962.6681078204...|[1.58569753097225...|       2.0|\n",
      "|Business|SAN JUAN, P.R. — ...|  2.0|[san, juan,, p.r....|[san, juan,, p.r....|(1000,[1,8,10,12,...|(1000,[1,8,10,12,...|[-5838.3449909497...|[5.13954664577655...|       1.0|\n",
      "|Business|SHANGHAI — Want t...|  2.0|[shanghai, —, wan...|[shanghai, —, wan...|(1000,[0,3,4,8,15...|(1000,[0,3,4,8,15...|[-3854.7002334727...|[2.04971544668225...|       2.0|\n",
      "|Business|SÃO PAULO, Brazil...|  2.0|[são, paulo,, bra...|[são, paulo,, bra...|(1000,[3,4,7,10,2...|(1000,[3,4,7,10,2...|[-2074.6773081437...|[1.87977674991846...|       2.0|\n",
      "|Business|TOKYO — The Japan...|  2.0|[tokyo, —, the, j...|[tokyo, —, japane...|(1000,[8,10,16,18...|(1000,[8,10,16,18...|[-1280.7713860671...|[3.65926739649113...|       2.0|\n",
      "|Business|The economy grew ...|  2.0|[the, economy, gr...|[economy, grew, a...|(1000,[3,7,8,10,1...|(1000,[3,7,8,10,1...|[-3530.7408637991...|[1.38550476759380...|       2.0|\n",
      "|Business|Wall Street was p...|  2.0|[wall, street, wa...|[wall, street, pr...|(1000,[3,25,34,36...|(1000,[3,25,34,36...|[-898.67269335400...|[0.00715648644266...|       2.0|\n",
      "|Politics|On Day 10 of a sc...|  0.0|[on, day, 10, of,...|[day, 10, scandal...|(1000,[0,1,2,5,6,...|(1000,[0,1,2,5,6,...|[-4028.5399298969...|[1.0,8.6713991623...|       0.0|\n",
      "|Politics|On Saturday, Rebe...|  0.0|[on, saturday,, r...|[saturday,, rebec...|(1000,[0,1,4,6,7,...|(1000,[0,1,4,6,7,...|[-4728.5781032164...|[0.99999999995944...|       0.0|\n",
      "|Politics|PALM BEACH, Fla. ...|  0.0|[palm, beach,, fl...|[palm, beach,, fl...|(1000,[0,3,6,7,10...|(1000,[0,3,6,7,10...|[-3787.1217369171...|[0.99999930512375...|       0.0|\n",
      "|Politics|PHOENIX — Melinda...|  0.0|[phoenix, —, meli...|[phoenix, —, meli...|(1000,[0,1,3,4,7,...|(1000,[0,1,3,4,7,...|[-5507.4545474465...|[1.0,1.0859097859...|       0.0|\n",
      "|Politics|The driver was dr...|  0.0|[the, driver, was...|[driver, drunk,, ...|(1000,[1,2,3,4,5,...|(1000,[1,2,3,4,5,...|[-4004.0763441880...|[0.99999998917304...|       0.0|\n",
      "|Politics|WASHINGTON — A fe...|  0.0|[washington, —, a...|[washington, —, h...|(1000,[0,1,3,4,5,...|(1000,[0,1,3,4,5,...|[-5028.4888714803...|[1.0,1.0514520189...|       0.0|\n",
      "|Politics|WASHINGTON — Even...|  0.0|[washington, —, e...|[washington, —, e...|(1000,[1,2,5,7,15...|(1000,[1,2,5,7,15...|[-3795.4997762411...|[1.0,8.2206181086...|       0.0|\n",
      "|Politics|WASHINGTON — Harr...|  0.0|[washington, —, h...|[washington, —, h...|(1000,[0,2,3,13,1...|(1000,[0,2,3,13,1...|[-2966.0111529850...|[1.0,1.5051879605...|       0.0|\n",
      "+--------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test set accuracy = 0.851063829787234\n"
     ]
    }
   ],
   "source": [
    "# applying Naive Bayes  \n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer , IDF , StringIndexer ,StopWordsRemover\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Configure an ML pipeline\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "stopwordsRemover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "hashingTF = HashingTF(inputCol=stopwordsRemover.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"features\")\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "pipeline = Pipeline(stages=[indexer, tokenizer,stopwordsRemover, hashingTF,idf, nb])\n",
    "\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=4)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModelNB = crossval.fit(training)\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "predictions = cvModelNB.transform(test)\n",
    "predictions.show()\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|category|                text|label|               words|            filtered|         rawFeatures|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|Business| Good Tuesday. He...|  2.0|[, good, tuesday....|[, good, tuesday....|(1000,[0,1,2,3,4,...|(1000,[0,1,2,3,4,...|[-0.7406603218313...|[0.00401937982547...|       2.0|\n",
      "|Business|Brady Hill used h...|  2.0|[brady, hill, use...|[brady, hill, use...|(1000,[0,1,2,3,8,...|(1000,[0,1,2,3,8,...|[-0.1155582931381...|[0.08386980169683...|       2.0|\n",
      "|Business|Good Wednesday. H...|  2.0|[good, wednesday....|[good, wednesday....|(1000,[0,1,2,3,4,...|(1000,[0,1,2,3,4,...|[0.51101972775904...|[0.01807309236991...|       2.0|\n",
      "|Business|I didn’t mean to ...|  2.0|[i, didn’t, mean,...|[didn’t, mean, it...|(1000,[0,2,4,6,7,...|(1000,[0,2,4,6,7,...|[-0.3644393046674...|[0.06680863860525...|       1.0|\n",
      "|Business|Long before dawn ...|  2.0|[long, before, da...|[long, dawn, wind...|(1000,[0,1,2,4,5,...|(1000,[0,1,2,4,5,...|[0.13112565541822...|[0.07124476241056...|       1.0|\n",
      "|Business|Microsoft, more t...|  2.0|[microsoft,, more...|[microsoft,, trad...|(1000,[0,1,8,13,1...|(1000,[0,1,8,13,1...|[-0.6630936518893...|[0.05464246643182...|       1.0|\n",
      "|Business|SAN JUAN, P.R. — ...|  2.0|[san, juan,, p.r....|[san, juan,, p.r....|(1000,[1,8,10,12,...|(1000,[1,8,10,12,...|[0.65130541633843...|[0.14458847592453...|       1.0|\n",
      "|Business|SHANGHAI — Want t...|  2.0|[shanghai, —, wan...|[shanghai, —, wan...|(1000,[0,3,4,8,15...|(1000,[0,3,4,8,15...|[0.07811656860139...|[0.02531448555066...|       2.0|\n",
      "|Business|SÃO PAULO, Brazil...|  2.0|[são, paulo,, bra...|[são, paulo,, bra...|(1000,[3,4,7,10,2...|(1000,[3,4,7,10,2...|[-0.0317095050174...|[0.09078047173183...|       2.0|\n",
      "|Business|TOKYO — The Japan...|  2.0|[tokyo, —, the, j...|[tokyo, —, japane...|(1000,[8,10,16,18...|(1000,[8,10,16,18...|[-0.4401049258937...|[0.13687667173840...|       2.0|\n",
      "|Business|The economy grew ...|  2.0|[the, economy, gr...|[economy, grew, a...|(1000,[3,7,8,10,1...|(1000,[3,7,8,10,1...|[0.15429064916140...|[0.04279056313077...|       2.0|\n",
      "|Business|Wall Street was p...|  2.0|[wall, street, wa...|[wall, street, pr...|(1000,[3,25,34,36...|(1000,[3,25,34,36...|[0.26461591218756...|[0.25414026762762...|       2.0|\n",
      "|Politics|On Day 10 of a sc...|  0.0|[on, day, 10, of,...|[day, 10, scandal...|(1000,[0,1,2,5,6,...|(1000,[0,1,2,5,6,...|[3.89912028926673...|[0.95397863981493...|       0.0|\n",
      "|Politics|On Saturday, Rebe...|  0.0|[on, saturday,, r...|[saturday,, rebec...|(1000,[0,1,4,6,7,...|(1000,[0,1,4,6,7,...|[1.82422692665061...|[0.59151645481913...|       0.0|\n",
      "|Politics|PALM BEACH, Fla. ...|  0.0|[palm, beach,, fl...|[palm, beach,, fl...|(1000,[0,3,6,7,10...|(1000,[0,3,6,7,10...|[1.88353047710433...|[0.54602602592313...|       0.0|\n",
      "|Politics|PHOENIX — Melinda...|  0.0|[phoenix, —, meli...|[phoenix, —, meli...|(1000,[0,1,3,4,7,...|(1000,[0,1,3,4,7,...|[3.33161028291339...|[0.85195987171896...|       0.0|\n",
      "|Politics|The driver was dr...|  0.0|[the, driver, was...|[driver, drunk,, ...|(1000,[1,2,3,4,5,...|(1000,[1,2,3,4,5,...|[0.95398252991273...|[0.24521930687820...|       2.0|\n",
      "|Politics|WASHINGTON — A fe...|  0.0|[washington, —, a...|[washington, —, h...|(1000,[0,1,3,4,5,...|(1000,[0,1,3,4,5,...|[4.62368863310841...|[0.97344060495980...|       0.0|\n",
      "|Politics|WASHINGTON — Even...|  0.0|[washington, —, e...|[washington, —, e...|(1000,[1,2,5,7,15...|(1000,[1,2,5,7,15...|[3.37881161366650...|[0.92797444781456...|       0.0|\n",
      "|Politics|WASHINGTON — Harr...|  0.0|[washington, —, h...|[washington, —, h...|(1000,[0,2,3,13,1...|(1000,[0,2,3,13,1...|[2.54105806776414...|[0.80748258843656...|       0.0|\n",
      "+--------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test set accuracy = 0.851063829787234\n"
     ]
    }
   ],
   "source": [
    "# applying Logistic Regression  \n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer , IDF , StringIndexer ,StopWordsRemover\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Configure an ML pipeline\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "stopwordsRemover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "hashingTF = HashingTF(inputCol=stopwordsRemover.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "pipeline = Pipeline(stages=[indexer, tokenizer,stopwordsRemover, hashingTF,idf, lr])\n",
    "\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=4)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModelLR = crossval.fit(training)\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "predictions = cvModelLR.transform(test)\n",
    "predictions.show()\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUNNING THE MODELS GENERATED ON A TEST DATA WHICH HAS BEEN SEPARATELY OBTAINED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Directory which consists the test data \n",
    "test_dir = \"Testing_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates a subdirectory list \n",
    "subDirNameList =[]\n",
    "for root, dirs, files in os.walk(dir, topdown=False):\n",
    "    for name in dirs:\n",
    "        subDirNameList.append(os.path.join(root, name))\n",
    "\n",
    "#creates a list of dictionary elements of each category type         \n",
    "listRDD = []\n",
    "dictRDDElementsAsMap ={}\n",
    "for eachFolder in subDirNameList:\n",
    "    folderName = os.path.basename(eachFolder)\n",
    "    rdd = sc.wholeTextFiles(eachFolder)\n",
    "    listRDD.append(rdd)\n",
    "    dictElement = rdd.collectAsMap()\n",
    "    dictRDDElementsAsMap[folderName] = dictElement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this creates a test list. Each element of a list is basically a dictionary : \n",
    "    key = category and\n",
    "    value = text content  \n",
    "\"\"\"\n",
    "testlistAll = []\n",
    "for name,v1 in dictRDDElementsAsMap.items():\n",
    "    for key,value in v1.items():\n",
    "        dataDic = {}\n",
    "        if isNotBlank(value):\n",
    "            dataDic['category'] = name\n",
    "            dataDic['text'] = value \n",
    "            testlistAll.append(dataDic)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suhit\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:340: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    }
   ],
   "source": [
    "# this creates the test dataframe from the testlistAll \n",
    "testDf = spark.createDataFrame(testlistAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+----------+\n",
      "|category|                text|label|               words|            filtered|         rawFeatures|            features|     rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+----------+\n",
      "|Business|One of the few re...|  2.0|[one, of, the, fe...|[one, remaining, ...|(1000,[0,2,3,9,11...|(1000,[0,2,3,9,11...| [1.0,1.0,9.0,4.0]|[0.06666666666666...|       2.0|\n",
      "|Business|Deutsche Bank sai...|  2.0|[deutsche, bank, ...|[deutsche, bank, ...|(1000,[2,8,12,20,...|(1000,[2,8,12,20,...|[0.0,2.0,12.0,1.0]|[0.0,0.1333333333...|       2.0|\n",
      "|Business|Spotify is a hit....|  2.0|[spotify, is, a, ...|[spotify, hit.on,...|(1000,[0,5,10,16,...|(1000,[0,5,10,16,...|[0.0,1.0,13.0,1.0]|[0.0,0.0666666666...|       2.0|\n",
      "|Business| Good Tuesday. He...|  2.0|[, good, tuesday....|[, good, tuesday....|(1000,[0,1,2,3,4,...|(1000,[0,1,2,3,4,...| [2.0,5.0,6.0,2.0]|[0.13333333333333...|       2.0|\n",
      "|Business|After days of som...|  2.0|[after, days, of,...|[days, sometimes,...|(1000,[1,12,13,14...|(1000,[1,12,13,14...|[0.0,1.0,13.0,1.0]|[0.0,0.0666666666...|       2.0|\n",
      "|Business|Good Thursday. He...|  2.0|[good, thursday.,...|[good, thursday.,...|(1000,[0,1,2,3,5,...|(1000,[0,1,2,3,5,...| [2.0,3.0,8.0,2.0]|[0.13333333333333...|       2.0|\n",
      "|Business|ABINGTON, Pa. — S...|  2.0|[abington,, pa., ...|[abington,, pa., ...|(1000,[0,1,6,8,13...|(1000,[0,1,6,8,13...| [0.0,1.0,9.0,5.0]|[0.0,0.0666666666...|       2.0|\n",
      "|Business|Wall Street was p...|  2.0|[wall, street, wa...|[wall, street, pr...|(1000,[3,25,34,36...|(1000,[3,25,34,36...| [1.0,2.0,7.0,5.0]|[0.06666666666666...|       2.0|\n",
      "|Business|SAN JUAN, P.R. — ...|  2.0|[san, juan,, p.r....|[san, juan,, p.r....|(1000,[1,8,10,12,...|(1000,[1,8,10,12,...| [4.0,3.0,3.0,5.0]|[0.26666666666666...|       3.0|\n",
      "|Business|The Labor Departm...|  2.0|[the, labor, depa...|[labor, departmen...|(1000,[0,1,2,11,1...|(1000,[0,1,2,11,1...|[0.0,1.0,13.0,1.0]|[0.0,0.0666666666...|       2.0|\n",
      "|Business|The sell-off in s...|  2.0|[the, sell-off, i...|[sell-off, stocks...|(1000,[8,25,30,34...|(1000,[8,25,30,34...| [0.0,1.0,9.0,5.0]|[0.0,0.0666666666...|       2.0|\n",
      "|Business|SHENZHEN, China —...|  2.0|[shenzhen,, china...|[shenzhen,, china...|(1000,[3,4,6,10,1...|(1000,[3,4,6,10,1...| [2.0,3.0,9.0,1.0]|[0.13333333333333...|       2.0|\n",
      "|Business|The stock market ...|  2.0|[the, stock, mark...|[stock, market, f...|(1000,[1,3,13,16,...|(1000,[1,3,13,16,...|[0.0,2.0,12.0,1.0]|[0.0,0.1333333333...|       2.0|\n",
      "|Business|I didn’t mean to ...|  2.0|[i, didn’t, mean,...|[didn’t, mean, it...|(1000,[0,2,4,6,7,...|(1000,[0,2,4,6,7,...| [6.0,7.0,2.0,0.0]|[0.4,0.4666666666...|       1.0|\n",
      "|Business|Brady Hill used h...|  2.0|[brady, hill, use...|[brady, hill, use...|(1000,[0,1,2,3,8,...|(1000,[0,1,2,3,8,...| [4.0,4.0,4.0,3.0]|[0.26666666666666...|       0.0|\n",
      "|Business|FRANKFURT — Mario...|  2.0|[frankfurt, —, ma...|[frankfurt, —, ma...|(1000,[1,3,5,11,1...|(1000,[1,3,5,11,1...|[3.0,1.0,10.0,1.0]|[0.2,0.0666666666...|       2.0|\n",
      "|Business|The value of the ...|  2.0|[the, value, of, ...|[value, dollar, f...|(1000,[8,10,13,14...|(1000,[8,10,13,14...|[0.0,3.0,11.0,1.0]|[0.0,0.2,0.733333...|       2.0|\n",
      "|Business|LONDON — It is th...|  2.0|[london, —, it, i...|[london, —, close...|(1000,[0,1,2,3,5,...|(1000,[0,1,2,3,5,...|[4.0,1.0,10.0,0.0]|[0.26666666666666...|       2.0|\n",
      "|Business|A start-up is tak...|  2.0|[a, start-up, is,...|[start-up, taking...|(1000,[13,15,23,2...|(1000,[13,15,23,2...|[0.0,2.0,11.0,2.0]|[0.0,0.1333333333...|       2.0|\n",
      "|Business|Target agreed on ...|  2.0|[target, agreed, ...|[target, agreed, ...|(1000,[0,7,13,17,...|(1000,[0,7,13,17,...| [2.0,4.0,9.0,0.0]|[0.13333333333333...|       2.0|\n",
      "+--------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Accuracy for Random Forest on new test data = 0.9318181818181818\n"
     ]
    }
   ],
   "source": [
    "# Running Random Forest on this new Test Data \n",
    "\n",
    "predictions = cvModelRF.transform(testDf)\n",
    "predictions.show()\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy for Random Forest on new test data = \" + str(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|category|                text|label|               words|            filtered|         rawFeatures|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|Business|One of the few re...|  2.0|[one, of, the, fe...|[one, remaining, ...|(1000,[0,2,3,9,11...|(1000,[0,2,3,9,11...|[-3011.9941663186...|[5.26919189748328...|       2.0|\n",
      "|Business|Deutsche Bank sai...|  2.0|[deutsche, bank, ...|[deutsche, bank, ...|(1000,[2,8,12,20,...|(1000,[2,8,12,20,...|[-2106.1071354629...|[3.46778455863961...|       2.0|\n",
      "|Business|Spotify is a hit....|  2.0|[spotify, is, a, ...|[spotify, hit.on,...|(1000,[0,5,10,16,...|(1000,[0,5,10,16,...|[-2521.2715558083...|[5.24177077111416...|       2.0|\n",
      "|Business| Good Tuesday. He...|  2.0|[, good, tuesday....|[, good, tuesday....|(1000,[0,1,2,3,4,...|(1000,[0,1,2,3,4,...|[-7506.8131448156...|[1.61820182577732...|       2.0|\n",
      "|Business|After days of som...|  2.0|[after, days, of,...|[days, sometimes,...|(1000,[1,12,13,14...|(1000,[1,12,13,14...|[-2191.7680300123...|[4.17544377601364...|       2.0|\n",
      "|Business|Good Thursday. He...|  2.0|[good, thursday.,...|[good, thursday.,...|(1000,[0,1,2,3,5,...|(1000,[0,1,2,3,5,...|[-8730.0431577621...|[1.80408137245065...|       2.0|\n",
      "|Business|ABINGTON, Pa. — S...|  2.0|[abington,, pa., ...|[abington,, pa., ...|(1000,[0,1,6,8,13...|(1000,[0,1,6,8,13...|[-4863.7543345193...|[1.94455957850438...|       2.0|\n",
      "|Business|Wall Street was p...|  2.0|[wall, street, wa...|[wall, street, pr...|(1000,[3,25,34,36...|(1000,[3,25,34,36...|[-898.67269335400...|[0.00715648644266...|       2.0|\n",
      "|Business|SAN JUAN, P.R. — ...|  2.0|[san, juan,, p.r....|[san, juan,, p.r....|(1000,[1,8,10,12,...|(1000,[1,8,10,12,...|[-5838.3449909497...|[5.13954664577655...|       1.0|\n",
      "|Business|The Labor Departm...|  2.0|[the, labor, depa...|[labor, departmen...|(1000,[0,1,2,11,1...|(1000,[0,1,2,11,1...|[-3190.5216782319...|[5.28421050630945...|       2.0|\n",
      "|Business|The sell-off in s...|  2.0|[the, sell-off, i...|[sell-off, stocks...|(1000,[8,25,30,34...|(1000,[8,25,30,34...|[-716.68137543547...|[2.15449959628509...|       2.0|\n",
      "|Business|SHENZHEN, China —...|  2.0|[shenzhen,, china...|[shenzhen,, china...|(1000,[3,4,6,10,1...|(1000,[3,4,6,10,1...|[-3709.6720756224...|[3.68395548400811...|       2.0|\n",
      "|Business|The stock market ...|  2.0|[the, stock, mark...|[stock, market, f...|(1000,[1,3,13,16,...|(1000,[1,3,13,16,...|[-2213.9265913997...|[3.59359840019174...|       2.0|\n",
      "|Business|I didn’t mean to ...|  2.0|[i, didn’t, mean,...|[didn’t, mean, it...|(1000,[0,2,4,6,7,...|(1000,[0,2,4,6,7,...|[-3860.7644305603...|[5.58602358666721...|       1.0|\n",
      "|Business|Brady Hill used h...|  2.0|[brady, hill, use...|[brady, hill, use...|(1000,[0,1,2,3,8,...|(1000,[0,1,2,3,8,...|[-4785.4573792874...|[2.64678005613299...|       1.0|\n",
      "|Business|FRANKFURT — Mario...|  2.0|[frankfurt, —, ma...|[frankfurt, —, ma...|(1000,[1,3,5,11,1...|(1000,[1,3,5,11,1...|[-3315.8439144248...|[4.76783878500862...|       2.0|\n",
      "|Business|The value of the ...|  2.0|[the, value, of, ...|[value, dollar, f...|(1000,[8,10,13,14...|(1000,[8,10,13,14...|[-2406.5359729508...|[3.84554733685896...|       2.0|\n",
      "|Business|LONDON — It is th...|  2.0|[london, —, it, i...|[london, —, close...|(1000,[0,1,2,3,5,...|(1000,[0,1,2,3,5,...|[-4295.1571068717...|[5.88172441745027...|       2.0|\n",
      "|Business|A start-up is tak...|  2.0|[a, start-up, is,...|[start-up, taking...|(1000,[13,15,23,2...|(1000,[13,15,23,2...|[-1589.2867134807...|[7.47630977571436...|       2.0|\n",
      "|Business|Target agreed on ...|  2.0|[target, agreed, ...|[target, agreed, ...|(1000,[0,7,13,17,...|(1000,[0,7,13,17,...|[-2684.6288165668...|[1.19133345808314...|       2.0|\n",
      "+--------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Accuracy for Naive Bayes on new test data = 0.9545454545454546\n"
     ]
    }
   ],
   "source": [
    "# Running Naive Bayes on this new Test Data \n",
    "\n",
    "predictions = cvModelNB.transform(testDf)\n",
    "predictions.show()\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy for Naive Bayes on new test data = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|category|                text|label|               words|            filtered|         rawFeatures|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|Business|One of the few re...|  2.0|[one, of, the, fe...|[one, remaining, ...|(1000,[0,2,3,9,11...|(1000,[0,2,3,9,11...|[-0.3172092509106...|[0.04569309638105...|       2.0|\n",
      "|Business|Deutsche Bank sai...|  2.0|[deutsche, bank, ...|[deutsche, bank, ...|(1000,[2,8,12,20,...|(1000,[2,8,12,20,...|[-0.4055738077416...|[0.06206982346279...|       2.0|\n",
      "|Business|Spotify is a hit....|  2.0|[spotify, is, a, ...|[spotify, hit.on,...|(1000,[0,5,10,16,...|(1000,[0,5,10,16,...|[-0.8795030717582...|[0.01198762837706...|       2.0|\n",
      "|Business| Good Tuesday. He...|  2.0|[, good, tuesday....|[, good, tuesday....|(1000,[0,1,2,3,4,...|(1000,[0,1,2,3,4,...|[-0.7406603218313...|[0.00401937982547...|       2.0|\n",
      "|Business|After days of som...|  2.0|[after, days, of,...|[days, sometimes,...|(1000,[1,12,13,14...|(1000,[1,12,13,14...|[-0.1337464066476...|[0.01928446558270...|       2.0|\n",
      "|Business|Good Thursday. He...|  2.0|[good, thursday.,...|[good, thursday.,...|(1000,[0,1,2,3,5,...|(1000,[0,1,2,3,5,...|[-0.0198318623180...|[0.00223920729712...|       2.0|\n",
      "|Business|ABINGTON, Pa. — S...|  2.0|[abington,, pa., ...|[abington,, pa., ...|(1000,[0,1,6,8,13...|(1000,[0,1,6,8,13...|[0.05852725101781...|[0.01848723712352...|       2.0|\n",
      "|Business|Wall Street was p...|  2.0|[wall, street, wa...|[wall, street, pr...|(1000,[3,25,34,36...|(1000,[3,25,34,36...|[0.26461591218756...|[0.25414026762762...|       2.0|\n",
      "|Business|SAN JUAN, P.R. — ...|  2.0|[san, juan,, p.r....|[san, juan,, p.r....|(1000,[1,8,10,12,...|(1000,[1,8,10,12,...|[0.65130541633843...|[0.14458847592453...|       1.0|\n",
      "|Business|The Labor Departm...|  2.0|[the, labor, depa...|[labor, departmen...|(1000,[0,1,2,11,1...|(1000,[0,1,2,11,1...|[-0.4710796609528...|[0.00716170624164...|       2.0|\n",
      "|Business|The sell-off in s...|  2.0|[the, sell-off, i...|[sell-off, stocks...|(1000,[8,25,30,34...|(1000,[8,25,30,34...|[-0.2617319971911...|[0.11287955191852...|       2.0|\n",
      "|Business|SHENZHEN, China —...|  2.0|[shenzhen,, china...|[shenzhen,, china...|(1000,[3,4,6,10,1...|(1000,[3,4,6,10,1...|[-0.0976212170915...|[0.02796134045170...|       2.0|\n",
      "|Business|The stock market ...|  2.0|[the, stock, mark...|[stock, market, f...|(1000,[1,3,13,16,...|(1000,[1,3,13,16,...|[-0.9466755644878...|[0.01670182023814...|       2.0|\n",
      "|Business|I didn’t mean to ...|  2.0|[i, didn’t, mean,...|[didn’t, mean, it...|(1000,[0,2,4,6,7,...|(1000,[0,2,4,6,7,...|[-0.3644393046674...|[0.06680863860525...|       1.0|\n",
      "|Business|Brady Hill used h...|  2.0|[brady, hill, use...|[brady, hill, use...|(1000,[0,1,2,3,8,...|(1000,[0,1,2,3,8,...|[-0.1155582931381...|[0.08386980169683...|       2.0|\n",
      "|Business|FRANKFURT — Mario...|  2.0|[frankfurt, —, ma...|[frankfurt, —, ma...|(1000,[1,3,5,11,1...|(1000,[1,3,5,11,1...|[0.10862511965586...|[0.05212680493397...|       2.0|\n",
      "|Business|The value of the ...|  2.0|[the, value, of, ...|[value, dollar, f...|(1000,[8,10,13,14...|(1000,[8,10,13,14...|[-0.3831358682494...|[0.04575677078512...|       2.0|\n",
      "|Business|LONDON — It is th...|  2.0|[london, —, it, i...|[london, —, close...|(1000,[0,1,2,3,5,...|(1000,[0,1,2,3,5,...|[0.01684002502551...|[0.01986400736163...|       2.0|\n",
      "|Business|A start-up is tak...|  2.0|[a, start-up, is,...|[start-up, taking...|(1000,[13,15,23,2...|(1000,[13,15,23,2...|[-0.2727928214479...|[0.08170405119475...|       2.0|\n",
      "|Business|Target agreed on ...|  2.0|[target, agreed, ...|[target, agreed, ...|(1000,[0,7,13,17,...|(1000,[0,7,13,17,...|[-0.0885195097157...|[0.06253463473132...|       2.0|\n",
      "+--------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Accuracy for Logistic Regression on new test data = 0.9636363636363636\n"
     ]
    }
   ],
   "source": [
    "# Running LR on this new Test Data \n",
    "\n",
    "predictions = cvModelLR.transform(testDf)\n",
    "predictions.show()\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy for Logistic Regression on new test data = \" + str(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
