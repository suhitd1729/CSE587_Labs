{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUHIT DATTA 50249271\n",
    "\n",
    "SOURAV RANU 50246451\n",
    "\n",
    "LAB 3 CSE 587"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code referenced from https://github.com/6chaoran/DataStory/blob/master/Titanic-Spark/pyspark-script.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: string (nullable = true)\n",
      " |-- Survived: double (nullable = true)\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- FirstName: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: double (nullable = true)\n",
      " |-- Parch: double (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- Mark: string (nullable = false)\n",
      "\n",
      "Train Data Number of Row: 637\n",
      "Validate Data Number of Row: 254\n",
      "Test Data Number of Row: 418\n"
     ]
    }
   ],
   "source": [
    "train_path='train.csv'\n",
    "test_path='test.csv'\n",
    "\n",
    "# Load csv file as RDD\n",
    "train_rdd = sc.textFile(train_path)\n",
    "test_rdd = sc.textFile(test_path)\n",
    "\n",
    "\n",
    "# Parse RDD to DF\n",
    "def parseTrain(rdd):\n",
    "\t\n",
    "\t# extract data header (first row)\n",
    "\theader = rdd.first()\n",
    "\t# remove header\n",
    "\tbody = rdd.filter(lambda r: r!=header)\n",
    "\n",
    "\tdef parseRow(row):\n",
    "\t\t# a function to parse each text row into\n",
    "\t\t# data format\n",
    "\n",
    "\t\t# remove double quote, split the text row by comma\n",
    "\t\trow_list = row.replace('\"','').split(\",\")\n",
    "\t\t# convert python list to tuple, which is \n",
    "\t\t# compatible with pyspark data structure\n",
    "\t\trow_tuple = tuple(row_list)\n",
    "\t\treturn row_tuple\n",
    "\n",
    "\trdd_parsed = body.map(parseRow)\n",
    "\n",
    "\tcolnames = header.split(\",\")\n",
    "\tcolnames.insert(3,'FirstName')\n",
    "\n",
    "\treturn rdd_parsed.toDF(colnames)\n",
    "\n",
    "def parseTest(rdd):\n",
    "\theader = rdd.first()\n",
    "\tbody = rdd.filter(lambda r: r!=header)\n",
    "\n",
    "\tdef parseRow(row):\n",
    "\t\trow_list = row.replace('\"','').split(\",\")\n",
    "\t\trow_tuple = tuple(row_list)\n",
    "\t\treturn row_tuple\n",
    "\n",
    "\trdd_parsed = body.map(parseRow)\n",
    "\n",
    "\tcolnames = header.split(\",\")\n",
    "\tcolnames.insert(2,'FirstName')\n",
    "\n",
    "\treturn rdd_parsed.toDF(colnames)\n",
    "\n",
    "train_df = parseTrain(train_rdd)\n",
    "test_df = parseTest(test_rdd)\n",
    "\n",
    "\n",
    "## Add Survived column to test\n",
    "## And append train/test data\n",
    "from pyspark.sql.functions import lit, col\n",
    "train_df = train_df.withColumn('Mark',lit('train'))\n",
    "test_df = (test_df.withColumn('Survived',lit(0))\n",
    "\t\t\t\t  .withColumn('Mark',lit('test')))\n",
    "\n",
    "test_df = test_df[train_df.columns]\n",
    "df = train_df.unionAll(test_df)\n",
    "\n",
    "## Data Cleaning/Manipulation\n",
    "## Convert Age, SibSp, Parch, Fare to Numeric\n",
    "df = (df.withColumn('Age',df['Age'].cast(\"double\"))\n",
    "\t\t\t.withColumn('SibSp',df['SibSp'].cast(\"double\"))\n",
    "\t\t\t.withColumn('Parch',df['Parch'].cast(\"double\"))\n",
    "\t\t\t.withColumn('Fare',df['Fare'].cast(\"double\"))\n",
    "\t\t\t.withColumn('Survived',df['Survived'].cast(\"double\"))\n",
    "\t\t\t)\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "## Impute missing Age and Fare\n",
    "numVars = ['Survived','Age','SibSp','Parch','Fare']\n",
    "def countNull(df,var):\n",
    "\treturn df.where(df[var].isNull()).count()\n",
    "\n",
    "missing = {var: countNull(df,var) for var in numVars}\n",
    "age_mean = df.groupBy().mean('Age').first()[0]\n",
    "fare_mean = df.groupBy().mean('Fare').first()[0]\n",
    "df = df.na.fill({'Age':age_mean,'Fare':fare_mean})\n",
    "\n",
    "\n",
    "# Feature Enginnering\n",
    "## 1. Extract Title from Name\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "## created user defined function to extract title\n",
    "getTitle = udf(lambda name: name.split('.')[0].strip(),StringType())\n",
    "df = df.withColumn('Title', getTitle(df['Name']))\n",
    "\n",
    "## 2. Index categorical variable\n",
    "catVars = ['Pclass','Sex','Embarked','Title']\n",
    "\n",
    "## index Sex variable\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "si = StringIndexer(inputCol = 'Sex', outputCol = 'Sex_indexed')\n",
    "df_indexed = si.fit(df).transform(df).drop('Sex').withColumnRenamed('Sex_indexed','Sex')\n",
    "\n",
    "## make use of pipeline to index all categorical variables\n",
    "def indexer(df,col):\n",
    "\tsi = StringIndexer(inputCol = col, outputCol = col+'_indexed').fit(df)\n",
    "\treturn si\n",
    "\n",
    "indexers = [indexer(df,col) for col in catVars]\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = indexers)\n",
    "df_indexed = pipeline.fit(df).transform(df)\n",
    "\n",
    "## 3. Convert to label/features format\n",
    "catVarsIndexed = [i+'_indexed' for i in catVars]\n",
    "featuresCol = numVars+catVarsIndexed\n",
    "featuresCol.remove('Survived')\n",
    "labelCol = ['Mark','Survived']\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "row = Row('mark','label','features')\n",
    "\n",
    "df_indexed = df_indexed[labelCol+featuresCol]\n",
    "# 0-mark, 1-label, 2-features\n",
    "lf = df_indexed.rdd.map(lambda r: (row(r[0], r[1],DenseVector(r[2:])))).toDF()\n",
    "# index label \n",
    "lf = StringIndexer(inputCol = 'label',outputCol='index').fit(lf).transform(lf)\n",
    "\n",
    "# split back train/test data\n",
    "train = lf.where(lf.mark =='train')\n",
    "test = lf.where(lf.mark =='test')\n",
    "\n",
    "# random split further to get train/validate\n",
    "train,validate = train.randomSplit([0.7,0.3],seed =121)\n",
    "\n",
    "print('Train Data Number of Row: '+ str(train.count()))\n",
    "print('Validate Data Number of Row: '+ str(validate.count()))\n",
    "print('Test Data Number of Row: '+ str(test.count()))\n",
    "\n",
    "# Apply Logsitic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# regPara: regualrization parameter\n",
    "lr = LogisticRegression(maxIter = 100, regParam = 0.05, labelCol='index').fit(train)\n",
    "\n",
    "# Evaluate model based on auc ROC(default for binary classification)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "def testModel(model, validate = validate):\n",
    "\tpred = model.transform(validate)\n",
    "\tevaluator = BinaryClassificationEvaluator(labelCol = 'index')\n",
    "\treturn evaluator.evaluate(pred)\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier\n",
    "\n",
    "\n",
    "dt = DecisionTreeClassifier(maxDepth = 3, labelCol ='index').fit(train)\n",
    "rf = RandomForestClassifier(numTrees = 100, labelCol = 'index').fit(train)\n",
    "\n",
    "\n",
    "models = {'LogisticRegression':lr,\n",
    "\t\t  'DecistionTree':dt,\n",
    "\t\t  'RandomForest':rf}\n",
    "\n",
    "modelPerf = {k:testModel(v) for k,v in models.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LogisticRegression': 0.8287225905150437, 'DecistionTree': 0.5850012748597654, 'RandomForest': 0.8541560428352879}\n"
     ]
    }
   ],
   "source": [
    "print(modelPerf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest gives the best performance for this data : which is 85.41 %\n",
    "    \n",
    "Logistic Regression gives the second best performance : accuracy of 82.87 %\n",
    "    \n",
    "Decision Tree gives an accuracy of 58.50 %"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
